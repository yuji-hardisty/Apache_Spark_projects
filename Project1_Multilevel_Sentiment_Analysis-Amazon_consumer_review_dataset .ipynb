{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilevel Sentiment Analysis using Spark \n",
    "# < Amazon consumer review dataset >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multilevel Sentiment Analysis from bag-of-words<br>\n",
    "- Libraries: Spark ml libraries\n",
    "- Dataset from https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navy'><font size=3.5>First, import csv type Amazon consumer review dataset as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String =./data/*.csv\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"./data/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: string, dateAdded: timestamp ... 22 more fields]\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dateAdded: timestamp (nullable = true)\n",
      " |-- dateUpdated: timestamp (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- asins: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- primaryCategories: string (nullable = true)\n",
      " |-- imageURLs: string (nullable = true)\n",
      " |-- keys: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- manufacturerNumber: string (nullable = true)\n",
      " |-- reviews.date: string (nullable = true)\n",
      " |-- reviews.dateSeen: string (nullable = true)\n",
      " |-- reviews.didPurchase: string (nullable = true)\n",
      " |-- reviews.doRecommend: boolean (nullable = true)\n",
      " |-- reviews.id: string (nullable = true)\n",
      " |-- reviews.numHelpful: integer (nullable = true)\n",
      " |-- reviews.rating: integer (nullable = true)\n",
      " |-- reviews.sourceURLs: string (nullable = true)\n",
      " |-- reviews.text: string (nullable = true)\n",
      " |-- reviews.title: string (nullable = true)\n",
      " |-- reviews.username: string (nullable = true)\n",
      " |-- sourceURLs: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navy'><font size=3.5> Among several data columns, I am going to use <br>\n",
    "<font color='navy'><font size=3.5> - review.rating: Label <br>\n",
    "<font color='navy'><font size=3.5> - review.text : Feature <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [text: string, label: int]\n"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var data = df.select(col(\"`reviews.text`\").as(\"text\"),col(\"`reviews.rating`\").as(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|I order 3 of them...|    3|\n",
      "|Bulk is always th...|    4|\n",
      "|Well they are not...|    5|\n",
      "|Seem to work as w...|    5|\n",
      "|These batteries a...|    5|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navy'><font size=3.5> Counts of each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|   44|    1|\n",
      "|    1|  965|\n",
      "|   16|    1|\n",
      "|    3| 1205|\n",
      "|    5|19831|\n",
      "|    4| 5621|\n",
      "|    2|  617|\n",
      "|    0|   91|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> --> <font color='navy'><font size=3.5> Rating is assumed to be from 0 to 5, therefore, the instance of label 44 and 16 are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res79: Long = 28332\n"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// size of dataframe\n",
    "data.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [text: string, label: int]\n"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.filter(data(\"label\") <= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res81: Long = 28330\n"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// size of dataframe after filter two rows\n",
    "data.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navy'><font size=3.5> Extracting and transforming features for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navy'><font size=3.5>   1) Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|                text|label|               words|\n",
      "+--------------------+-----+--------------------+\n",
      "|I order 3 of them...|    3|[i, order, 3, of,...|\n",
      "|Bulk is always th...|    4|[bulk, is, always...|\n",
      "|Well they are not...|    5|[well, they, are,...|\n",
      "|Seem to work as w...|    5|[seem, to, work, ...|\n",
      "|These batteries a...|    5|[these, batteries...|\n",
      "|Bought a lot of b...|    5|[bought, a, lot, ...|\n",
      "|ive not had any p...|    5|[ive, not, had, a...|\n",
      "|Well if you are l...|    5|[well, if, you, a...|\n",
      "|These do not hold...|    3|[these, do, not, ...|\n",
      "|AmazonBasics AA A...|    4|[amazonbasics, aa...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.RegexTokenizer\n",
       "regexTokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_85fe7adf7907, minTokenLength=1, gaps=true, pattern=\\s+, toLowercase=true\n",
       "words: org.apache.spark.sql.DataFrame = [text: string, label: int ... 1 more field]\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "val regexTokenizer = new RegexTokenizer()\n",
    "      .setInputCol(\"text\") \n",
    "      .setOutputCol(\"words\")\n",
    "val words = regexTokenizer.transform(data)\n",
    "words.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navy'><font size=3.5>  2) Stopwords remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|                text|label|               words|            filtered|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|I order 3 of them...|    3|[i, order, 3, of,...|[order, 3, one, i...|\n",
      "|Bulk is always th...|    4|[bulk, is, always...|[bulk, always, le...|\n",
      "|Well they are not...|    5|[well, they, are,...|[well, duracell, ...|\n",
      "|Seem to work as w...|    5|[seem, to, work, ...|[seem, work, well...|\n",
      "|These batteries a...|    5|[these, batteries...|[batteries, long,...|\n",
      "|Bought a lot of b...|    5|[bought, a, lot, ...|[bought, lot, bat...|\n",
      "|ive not had any p...|    5|[ive, not, had, a...|[ive, problame, b...|\n",
      "|Well if you are l...|    5|[well, if, you, a...|[well, looking, c...|\n",
      "|These do not hold...|    3|[these, do, not, ...|[hold, amount, hi...|\n",
      "|AmazonBasics AA A...|    4|[amazonbasics, aa...|[amazonbasics, aa...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = StopWordsRemover: uid=stopWords_d593ab0e5208, numStopWords=181, locale=en_US, caseSensitive=false\n",
       "words_filtered: org.apache.spark.sql.DataFrame = [text: string, label: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "val remover = new StopWordsRemover()\n",
    "      .setInputCol(\"words\")\n",
    "      .setOutputCol(\"filtered\")\n",
    "val words_filtered = remover.transform(words)\n",
    "words_filtered.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navy'><font size=3.5>  3) CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|                text|label|               words|            filtered|            features|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|I order 3 of them...|    3|[i, order, 3, of,...|[order, 3, one, i...|(5000,[12,22,83,9...|\n",
      "|Bulk is always th...|    4|[bulk, is, always...|[bulk, always, le...|(5000,[11,66,94,1...|\n",
      "|Well they are not...|    5|[well, they, are,...|[well, duracell, ...|(5000,[9,25,114,1...|\n",
      "|Seem to work as w...|    5|[seem, to, work, ...|[seem, work, well...|(5000,[1,9,17,25,...|\n",
      "|These batteries a...|    5|[these, batteries...|[batteries, long,...|(5000,[1,9,16,74,...|\n",
      "|Bought a lot of b...|    5|[bought, a, lot, ...|[bought, lot, bat...|(5000,[1,5,7,18,4...|\n",
      "|ive not had any p...|    5|[ive, not, had, a...|[ive, problame, b...|(5000,[1,354,768,...|\n",
      "|Well if you are l...|    5|[well, if, you, a...|[well, looking, c...|(5000,[1,14,25,12...|\n",
      "|These do not hold...|    3|[these, do, not, ...|[hold, amount, hi...|(5000,[11,27,208,...|\n",
      "|AmazonBasics AA A...|    4|[amazonbasics, aa...|[amazonbasics, aa...|(5000,[1,2,18,25,...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
       "cvModel: org.apache.spark.ml.feature.CountVectorizerModel = CountVectorizerModel: uid=cntVec_e7c62d8fe82e, vocabularySize=5000\n",
       "features: org.apache.spark.sql.DataFrame = [text: string, label: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// fit a CountVectorizerModel from the corpus\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "val cvModel: CountVectorizerModel = new CountVectorizer()\n",
    "  .setInputCol(\"filtered\")\n",
    "  .setOutputCol(\"features\")\n",
    "  .setVocabSize(5000)\n",
    "  .fit(words_filtered)\n",
    "val features = cvModel.transform(words_filtered)\n",
    "features.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navy'><font size=3.5>  Splitting data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [text: string, label: int]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [text: string, label: int]\n"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = data.randomSplit(Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navy'><font size=3.5>  Making above trasforming processes into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.RegexTokenizer\n",
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "import org.apache.spark.ml.feature.CountVectorizer\n",
       "import org.apache.spark.ml.Pipeline\n",
       "regexTokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_5095a85a2076, minTokenLength=1, gaps=true, pattern=\\s+, toLowercase=true\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = StopWordsRemover: uid=stopWords_216d0252aea2, numStopWords=181, locale=en_US, caseSensitive=false\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_5b8921938305\n"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val regexTokenizer = new RegexTokenizer()\n",
    "      .setInputCol(\"text\") \n",
    "      .setOutputCol(\"words\")\n",
    "\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "      .setInputCol(\"words\")\n",
    "      .setOutputCol(\"filtered\")\n",
    "                              \n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(regexTokenizer, remover))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_pipeline_model: org.apache.spark.ml.PipelineModel = pipeline_5b8921938305\n",
       "test_pipeline_model: org.apache.spark.ml.PipelineModel = pipeline_5b8921938305\n"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_pipeline_model = pipeline.fit(train)        \n",
    "val test_pipeline_model = pipeline.fit(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_filtered: org.apache.spark.sql.DataFrame = [text: string, label: int ... 2 more fields]\n",
       "test_filtered: org.apache.spark.sql.DataFrame = [text: string, label: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_filtered = train_pipeline_model.transform(train)        \n",
    "val test_filtered = test_pipeline_model.transform(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.CountVectorizer\n",
       "cvModel: org.apache.spark.ml.feature.CountVectorizerModel = CountVectorizerModel: uid=cntVec_a72fd53ee83d, vocabularySize=10000\n"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "val cvModel: CountVectorizerModel = new CountVectorizer()\n",
    "  .setInputCol(\"filtered\")\n",
    "  .setOutputCol(\"features\")\n",
    "  .setVocabSize(10000)\n",
    "  .fit(train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features_train: org.apache.spark.sql.DataFrame = [text: string, label: int ... 3 more fields]\n",
       "features_test: org.apache.spark.sql.DataFrame = [text: string, label: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val features_train = cvModel.transform(train_filtered)\n",
    "val features_test = cvModel.transform(test_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navy'><font size=3.5>  Apply text classification moel: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_0ad8c9e8e1a8\n",
       "model: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_0ad8c9e8e1a8, numClasses=45, numFeatures=10000\n"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.001)\n",
    "val model = lr.fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred: org.apache.spark.sql.DataFrame = [text: string, label: int ... 6 more fields]\n"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pred = model.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_0d66e226ed9c, metricName=accuracy, metricLabel=0.0, beta=1.0, eps=1.0E-15\n"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setMetricName(\"accuracy\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res54: Double = 0.790589912671538\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
